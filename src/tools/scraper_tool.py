"""
THE CRAWLER LIMB â€” SCRAPER TOOL
================================

ORGAN METAPHOR:
---------------
The Scraper is a LIMB of the research organism.
It reaches out to fetch web pages exactly as instructed.

ASCII DIAGRAM:
--------------
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   MISSION PLAN      â”‚
                     â”‚   Task: Fetch URL   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   SCRAPER TOOL      â”‚
                     â”‚      (LIMB)         â”‚
                     â”‚  [DETERMINISTIC]    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                        â•‘   INTERNET    â•‘
                        â•‘   ğŸŒ â†’ HTML   â•‘
                        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                â”‚
                                â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   RAW HTML OUTPUT   â”‚
                     â”‚   <html>...</html>  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AGENT vs TOOL:
--------------
This is a TOOL because it:
  â€¢ Performs deterministic fetching operations
  â€¢ Does not decide WHAT to fetch, only HOW to fetch it
  â€¢ Has no autonomous decision-making capability
  â€¢ Simply executes the fetch command and returns raw HTML

RESPONSIBILITIES:
-----------------
  1. Fetch raw HTML from a given URL
  2. Handle HTTP headers (user-agent, timeouts)
  3. Return raw page content or error status
  4. Log fetch metadata (status code, response time)

TEACHING NOTES:
---------------
The Scraper TOOL is purely mechanical. It's like a robotic arm â€” it does
exactly what it's told, with no judgment or planning. Compare this to
the Planner AGENT, which decides what to fetch in the first place.

In Phase 5, we'll introduce a SCRAPER AGENT that can autonomously decide
to retry, crawl linked pages, or choose alternate sources. This upgrade
demonstrates the transition from TOOL â†’ AGENT.

FUTURE EXTENSIONS:
------------------
  â€¢ Async fetching for concurrent requests
  â€¢ Retry logic with exponential backoff
  â€¢ Caching to avoid redundant fetches
  â€¢ robots.txt compliance checking

DEBUGGING TIPS:
---------------
  â€¢ Log all fetch attempts with timestamps
  â€¢ Monitor for rate limiting or blocked requests
  â€¢ Track success/failure rates by domain
"""

from typing import Tuple, Optional


def fetch_url(url: str, timeout: int = 30) -> Tuple[Optional[str], int, str]:
    """
    Fetch raw HTML content from a URL.

    This is a TOOL FUNCTION â€” deterministic, no decision-making.
    It simply executes the fetch and returns the result.

    Args:
        url: The URL to fetch
        timeout: Request timeout in seconds

    Returns:
        Tuple of (html_content, status_code, error_message)
        - html_content: Raw HTML string, or None if fetch failed
        - status_code: HTTP status code (200, 404, 500, etc.)
        - error_message: Empty string on success, error details on failure

    Example:
        >>> html, status, error = fetch_url("https://example.com")
        >>> if status == 200:
        >>>     print(f"Fetched {len(html)} characters")
        >>> else:
        >>>     print(f"Failed: {error}")

    TEACHING NOTE:
    --------------
    This is a stub implementation. In Phase 2, we'll add:
      â€¢ Actual HTTP request using requests library
      â€¢ User-agent headers to identify our scraper
      â€¢ Error handling for network failures
      â€¢ Response time tracking
    """
    # STUB: Return placeholder data
    # In Phase 2, this will be replaced with actual HTTP requests

    print(f"[SCRAPER TOOL] Fetching URL: {url}")

    # Placeholder return â€” simulates a successful fetch
    placeholder_html = f"""
    <html>
        <head><title>Placeholder Content</title></head>
        <body>
            <h1>This is stub content for: {url}</h1>
            <p>In Phase 2, this will be real HTML fetched from the web.</p>
        </body>
    </html>
    """

    return (placeholder_html, 200, "")


# FUTURE: Add these functions in Phase 2
# def fetch_with_retry(url: str, max_retries: int = 3) -> ...:
#     """Fetch with exponential backoff retry logic"""
#     pass
#
# def check_robots_txt(url: str) -> bool:
#     """Check if URL is allowed by robots.txt"""
#     pass
